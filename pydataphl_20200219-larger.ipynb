{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to using GPUs for Analytics\n",
    "\n",
    "Speaker: [Randy Zwitch](https://github.com/randyzwitch), Senior Director of Community at [OmniSci](https://www.omnisci.com/) <br>\n",
    "PyData PHL: https://www.meetup.com/PyData-PHL/events/268253667/ <br>\n",
    "Feb 18, 2020 <br>\n",
    "\n",
    "This notebook demostrates some of the basic principles for using GPUs to accelerate computations. It is not intended to be a primer on machine learning; rather, the intent is to help users gain an intuition about code that can be parallelized in general, then show the speed up from moving computation from CPU to GPU.\n",
    "\n",
    "_Edit 2/19/2020: This example has been modified for much larger hardware (AMD 3970 w/ 128GB RAM, NVIDIA 1080ti) to make the comparisons clearer._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(295854, 15)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#1 month of bikshare data from Baywheels (SF)\n",
    "#~295k records not that large, but useful for example\n",
    "#full dataset: https://s3.amazonaws.com/baywheels-data/index.html\n",
    "baywheels_df = pd.read_csv(\"https://s3.amazonaws.com/baywheels-data/202001-baywheels-tripdata.csv.zip\", low_memory=False)\n",
    "baywheels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make baywheels_df a lot larger\n",
    "#re-using the baywheels_df binding to make code re-run simpler\n",
    "for i in range(1,9):\n",
    "    baywheels_df = baywheels_df.append(baywheels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75738624, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baywheels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>bike_id</th>\n",
       "      <th>user_type</th>\n",
       "      <th>bike_share_for_all_trip</th>\n",
       "      <th>rental_access_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83118</td>\n",
       "      <td>2020-01-31 15:23:47.7330</td>\n",
       "      <td>2020-02-01 14:29:06.2630</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Buchanan St at North Point St</td>\n",
       "      <td>37.804272</td>\n",
       "      <td>-122.433537</td>\n",
       "      <td>400.0</td>\n",
       "      <td>Buchanan St at North Point St</td>\n",
       "      <td>37.804272</td>\n",
       "      <td>-122.433537</td>\n",
       "      <td>13052</td>\n",
       "      <td>Customer</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68239</td>\n",
       "      <td>2020-01-31 15:40:31.6160</td>\n",
       "      <td>2020-02-01 10:37:51.0000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Folsom St at 15th St</td>\n",
       "      <td>37.767037</td>\n",
       "      <td>-122.415443</td>\n",
       "      <td>99.0</td>\n",
       "      <td>Folsom St at 15th St</td>\n",
       "      <td>37.767037</td>\n",
       "      <td>-122.415443</td>\n",
       "      <td>12235</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55407</td>\n",
       "      <td>2020-01-31 17:48:16.6350</td>\n",
       "      <td>2020-02-01 09:11:44.3170</td>\n",
       "      <td>197.0</td>\n",
       "      <td>El Embarcadero at Grand Ave</td>\n",
       "      <td>37.808848</td>\n",
       "      <td>-122.249680</td>\n",
       "      <td>197.0</td>\n",
       "      <td>El Embarcadero at Grand Ave</td>\n",
       "      <td>37.808848</td>\n",
       "      <td>-122.249680</td>\n",
       "      <td>12822</td>\n",
       "      <td>Customer</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54929</td>\n",
       "      <td>2020-01-31 17:53:03.4130</td>\n",
       "      <td>2020-02-01 09:08:32.6340</td>\n",
       "      <td>197.0</td>\n",
       "      <td>El Embarcadero at Grand Ave</td>\n",
       "      <td>37.808848</td>\n",
       "      <td>-122.249680</td>\n",
       "      <td>197.0</td>\n",
       "      <td>El Embarcadero at Grand Ave</td>\n",
       "      <td>37.808848</td>\n",
       "      <td>-122.249680</td>\n",
       "      <td>11705</td>\n",
       "      <td>Customer</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55700</td>\n",
       "      <td>2020-01-31 17:12:33.4600</td>\n",
       "      <td>2020-02-01 08:40:53.6460</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Pier 1/2 at The Embarcadero</td>\n",
       "      <td>37.796389</td>\n",
       "      <td>-122.394586</td>\n",
       "      <td>371.0</td>\n",
       "      <td>Lombard St at Columbus Ave</td>\n",
       "      <td>37.802746</td>\n",
       "      <td>-122.413579</td>\n",
       "      <td>3673</td>\n",
       "      <td>Customer</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration_sec                start_time                  end_time  \\\n",
       "0         83118  2020-01-31 15:23:47.7330  2020-02-01 14:29:06.2630   \n",
       "1         68239  2020-01-31 15:40:31.6160  2020-02-01 10:37:51.0000   \n",
       "2         55407  2020-01-31 17:48:16.6350  2020-02-01 09:11:44.3170   \n",
       "3         54929  2020-01-31 17:53:03.4130  2020-02-01 09:08:32.6340   \n",
       "4         55700  2020-01-31 17:12:33.4600  2020-02-01 08:40:53.6460   \n",
       "\n",
       "   start_station_id             start_station_name  start_station_latitude  \\\n",
       "0             400.0  Buchanan St at North Point St               37.804272   \n",
       "1              99.0           Folsom St at 15th St               37.767037   \n",
       "2             197.0    El Embarcadero at Grand Ave               37.808848   \n",
       "3             197.0    El Embarcadero at Grand Ave               37.808848   \n",
       "4              12.0    Pier 1/2 at The Embarcadero               37.796389   \n",
       "\n",
       "   start_station_longitude  end_station_id               end_station_name  \\\n",
       "0              -122.433537           400.0  Buchanan St at North Point St   \n",
       "1              -122.415443            99.0           Folsom St at 15th St   \n",
       "2              -122.249680           197.0    El Embarcadero at Grand Ave   \n",
       "3              -122.249680           197.0    El Embarcadero at Grand Ave   \n",
       "4              -122.394586           371.0     Lombard St at Columbus Ave   \n",
       "\n",
       "   end_station_latitude  end_station_longitude  bike_id   user_type  \\\n",
       "0             37.804272            -122.433537    13052    Customer   \n",
       "1             37.767037            -122.415443    12235  Subscriber   \n",
       "2             37.808848            -122.249680    12822    Customer   \n",
       "3             37.808848            -122.249680    11705    Customer   \n",
       "4             37.802746            -122.413579     3673    Customer   \n",
       "\n",
       "  bike_share_for_all_trip rental_access_method  \n",
       "0                      No                  NaN  \n",
       "1                     Yes                  NaN  \n",
       "2                      No                  NaN  \n",
       "3                      No                  NaN  \n",
       "4                      No                  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baywheels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU, single-threaded computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. For Loop\n",
    "\n",
    "While __for loops__ are very powerful, in an interpreted language like Python, this will rarely be the _fastest_ way to perform a calculation. Especially for analytics, where the same calculation is applied to thousands, millions, or billions of data elements.\n",
    "\n",
    "for loops allow for accessing and writing to global variables; because anything can change these global variables, the interpreter can't assume anything about the data in terms of optimizations. The extreme flexibility of for loops works against their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 s ± 58.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "minutes = []\n",
    "for val in baywheels_df[\"duration_sec\"]:\n",
    "    minutes.append(val / 60)\n",
    "    \n",
    "baywheels_df[\"duration_min_loop\"] = minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. List Comprehension\n",
    "\n",
    "A list comprehension in Python is more frequently seen to apply a function over an array. Here, because list comprehension knows the size of the input (`baywheels_df[\"duration_sec\"]`), the list comprehension can 1) allocate the full memory needed to hold the output list at once and 2) given a list comprehension has a smaller surface area of what it can do, the code _can be_ more specialized (but this depends on the Python implementation).\n",
    "\n",
    "In this example, the list comprehension is roughly 15% faster than a for loop. You can still do some weird things with list comprehensions (run functions that only have side-effects, filtering, etc.), but they are still more predictable than a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8 s ± 82.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "baywheels_df[\"duration_min_lc\"] = [x/60 for x in baywheels_df[\"duration_sec\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using NumPy/pandas \"vectorization\"\n",
    "\n",
    "Extending the idea of \"smaller surface area\" functions being able to be more optimized at the expense of flexibility, \"vectorized\" calculations have the same properties as list comprehensions (i.e. input/output size known at function time). NumPy goes one step further, being written in C, which allows for a 57x speedup (11.8 seconds vs 207 _milliseconds_).\n",
    "\n",
    "However, once you \"drop to C\" (or other compiled languages), you eventually run out of ability to go any faster. One thread eventually hits the maximum amount of work possibly for a given CPU clock speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 ms ± 138 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "baywheels_df[\"duration_min_vec\"] = baywheels_df[\"duration_sec\"] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using multiprocessing\n",
    "\n",
    "If you run out of single-threaded performance, the answer _must be_ MOAR THREADS, right? Not necessarily...\n",
    "\n",
    "In the example below, I use `multiprocessing` to create 6 workers to process this data. Given that Python has some limitations in terms of threading due to the [Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock), `multiprocessing` _forks_ (i.e. makes separate copies) the main environment. This is an expensive operation in order to guarantee safety of the parallel processing operation, so you need to be doing A LOT of work to make it worth it. Some IO operations, making database calls, etc. can be useful to parallelize using `multiprocessing` (as Python isn't the limiting factor for these operations).\n",
    "\n",
    "In addition to making copies of the environment, we are still running interpreted Python (in parallel) vs. compiled C. NumPy/pandas/C single-threaded outperforms Python by such a large margin that parallelism isn't enough to overcome the interpreted Python code, even with 32x as many workers are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def to_minute(x):\n",
    "    return x/60\n",
    "\n",
    "p = Pool(32) #have a 32-core demo machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 s ± 96.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "baywheels_df[\"duration_min_pool\"] = p.map(to_minute, baywheels_df[\"duration_sec\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why can GPUs be useful for analytics?\n",
    "\n",
    "Many analytics use cases are highly parallelizable. When calculating sales by zip code, average salary by age, or any other \"group by\" type of operation, the results of one group isn't determined by or based on any other group. GPUs are set up to apply simple arithmetic (often called 'kernels') to thousands of data elements in parallel. \n",
    "\n",
    "That GPUs usually have a lower clock speed than CPUs is besides the point; the massive parallelism of GPUs far overshadows the lower amount of work a single-thread might be doing. Think of a CPU vs. GPU as a similar comparison of a car vs. a bus...\n",
    "\n",
    "A car might be able to drive around a track with 4 people inside it in 100 seconds. It will take the car 1000 seconds to move 40 people around the track (ignoring loading times). A bus might drive a lap in 200 seconds (half as fast), but carry 40 people. In this case, it takes the bus 200 seconds to move 40 people, __5x faster in clock time__ than the car. GPUs can be viewed as maximizing _throughput_, not the speed of a single process like a CPU.\n",
    "\n",
    "Of course, in the case of CPUs vs. GPUs, the throughput disparity is even higher that a car vs. a bus. Even in this consumer laptop, I have 640 GPU cores vs. 6 CPU cores; a high-end GPU might have 4000-5000 GPU cores, and computations can be parallelized across multiple GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CuPy\n",
    "\n",
    "NumPy has been indispensable in bringing Python to the scientific community. By mixing typed arrays and high-performance code written in C, NumPy overcomes a lot of the issues with using interpreted programming languages for analysis (while not bogging the user down with the compile-debug-run workload).\n",
    "\n",
    "Because of NumPy's success, its API has been implemented in other array focused libraries. [CuPy](https://cupy.chainer.org/) continues this tradition, allwoing for using NumPy-like syntax to run against NVIDIA GPUs.\n",
    "\n",
    "In the example below, even doing only a minimal amount of work on the GPU (array of 295k elements) is __13x faster__ compared to the \"vectorized\" CPU NumPy pandas example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 19 11:08:20 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 108...  Off  | 00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   43C    P8     9W / 250W |    147MiB / 11178MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce GTX 108...  Off  | 00000000:21:00.0  On |                  N/A |\n",
      "| 18%   53C    P8    15W / 250W |   1437MiB / 11175MiB |      7%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1880      C   /home/rzwitch/omnisci/bin/omnisci_server     135MiB |\n",
      "|    1      1880      C   /home/rzwitch/omnisci/bin/omnisci_server     135MiB |\n",
      "|    1      2428      G   /usr/lib/xorg/Xorg                           604MiB |\n",
      "|    1      3249      G   /usr/bin/gnome-shell                         196MiB |\n",
      "|    1     18729      G   ...uest-channel-token=17095772474108565196   165MiB |\n",
      "|    1     20043      G   ...quest-channel-token=4558746828780586468    35MiB |\n",
      "|    1     31533      G   ...quest-channel-token=9926527032110441889   169MiB |\n",
      "|    1     34760      G   ...AAAAAAAAAAAAAAgAAAAAAAAA --shared-files   125MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Check that an NVIDIA GPU is running locally\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "#Bring data from pandas dataframe to NVIDIA GPU\n",
    "#THIS COPYING IS NOT FREE! Like multiprocessing, need to make sure you are doing \"enough work\" to make this worthwhile\n",
    "#Copying to GPU however is orders of magnitude faster than forking new Python processes\n",
    "duration_sec_cp = cp.asarray(baywheels_df[\"duration_sec\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12 ms ± 677 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "#Same basic operation as the NumPy pandas example, except 66x faster\n",
    "duration_min_cp =  duration_sec_cp/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cupy.core.core.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(duration_sec_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. cuDF\n",
    "\n",
    "Of course, if you're not doing linear algebra, there's no reason to use CuPy (even if it's a drop-in replacement for NumPy). [cuDF](https://github.com/rapidsai/cudf) attempts to mimic pandas for operations, so that moving from CPU to GPU is as frictionless as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "#Like transferring data with CuPy, this is not a \"free\" operation\n",
    "#Need to make sure enough work will be done to make the memory transfer worth it\n",
    "#baywheels_df_gpu = cudf.from_pandas(baywheels_df)\n",
    "baywheels_df_gpu = cudf.from_pandas(baywheels_df[[\"duration_sec\"]]) #transferring single column as dataframe for GPU memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cudf.core.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Can validate the type of the object, since it can be confusing!\n",
    "type(baywheels_df_gpu)\n",
    "#baywheels_df_gpu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684 ms ± 4.23 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "#This ends up being slower that CPU, likely because there isn't \"enough work\" vs highly optimized NumPy code\n",
    "#or possibly because CPU is more efficient in processing integer inputs\n",
    "#still considerably faster than Python list comprehension of 11.8 seconds\n",
    "baywheels_df_gpu[\"duration_min_gpu\"] = baywheels_df[\"duration_sec\"] / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "In this talk, I've intentionally kept the discussion high-level, in order to help build intuition around operations that can be parallelized. These toy examples don't necessarily highlight the maximum performance you would see moving a CPU workload to a GPU, but it shows that very little GPU-specific knowledge is needed to get started.\n",
    "\n",
    "Once you move beyond CuPy and cuDF as drop-in replacements for NumPy and pandas, things can become more complex. Writing CUDA kernels using Numba or PyCUDA leads to code that necessarily starts to look lower-level than most Python you will see. You will also need a strong understanding of associative and commutatitve operations, as well as how to keep GPU threads synchronized for different steps of a kernel. This is of course beyond the scope of this beginner talk, but if you are interested in learning that, both [NVIDIA](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) and [Numba](https://numba.pydata.org/numba-doc/latest/cuda/kernels.html) provide great learning materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
